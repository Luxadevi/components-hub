name: scrape
run-name: Scrape components and write to file
on: 
  # TODO: Need to make sure this doesn't trigger a rerun when the automatic PR is merged.
  push:
    branches:
      - snowvation
  schedule:
    - cron: "0 0 * * *"  # runs at 00:00 UTC every day
  # TODO: Must be in the default branch according to docs.
  workflow_dispatch:
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo content
        uses: actions/checkout@v2
      - name: Setup python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install pipenv
        run: |
          python -m pip install --upgrade pipenv wheel
      - name: Use cached dependencies if available
        id: cache-pipenv
        uses: actions/cache@v1
        with:
          path: ~/.local/share/virtualenvs
          key: ${{ runner.os }}-pipenv-${{ hashFiles('**/Pipfile.lock') }}
      - name: Install dependencies
        if: steps.cache-pipenv.outputs.cache-hit != 'true'
        run: |
          pipenv install
      - name: Execute scraper script
        run: |
          pipenv run python scraper.py
        env: 
          GH_TOKEN: ${{ secrets.GH_TOKEN }}
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v4
        with:
          delete-branch: true
          title: ðŸ¤– Latest scraper run
          body: |
            Hey there ðŸ‘‹ Here are the changes from the latest run of the components scraper in `scraper.py`. This is an automatic PR created via Github Actions. 

            Check the **Files changed** tab at the top to see which components were updated. 
          branch: latest-scraper-run
          commit-message: ðŸ¤– Add updates from latest scraper run
          author: GitHub <noreply@github.com>
